# Veridion – Company Classifier for Insurance Taxonomy

A simple and explainable project that assigns one or more insurance-related labels to each company from a fixed taxonomy.

Goal: take the companies table and return an “annotated” table with a new `insurance_label` column that you can present and validate easily.

---

## 1) What it does
- Reads companies from `ml_insurance_challenge.csv` (columns: `description`, `business_tags`, `sector`, `category`, `niche`).
- Reads the taxonomy from `insurance_taxonomy - insurance_taxonomy.csv` (column: `label`).
- For each company, scores all taxonomy labels and selects the best one(s).
- Writes `annotated_ml_insurance_challenge.csv` with:
  - `insurance_label` – selected labels (separated by `;`).
  - `label_scores_top5` – top 5 labels with scores (JSON) for auditability.

---

## 2) How it works (plain language)
We blend two signals:
1) Simple rules (rule-based)
   - Look for phrase and word overlaps between the label and company text (description + tags + sector/category/niche).
   - Measure token overlap with the company’s metadata.

2) TF‑IDF (text similarity)
   - Convert texts to vectors and compute cosine similarity between the company doc and the label.
   - Use “field‑aware TF‑IDF”: compute similarity separately for
     - description (`description`), and
     - meta+tags (`business_tags + sector + category + niche`),
     then combine them with weights.

Final score = weighted mix of rules and TF‑IDF. We pick labels near the best score using a dynamic threshold, so we don’t miss very close alternatives.

---

## 3) Project files
- `main.py` – the classifier (reads CSVs, scores, writes output).
- `report.py` – small helper that shows label distribution and ambiguous cases.
- `requirements.txt` – Python dependencies.
- `ml_insurance_challenge.csv` – companies (input).
- `insurance_taxonomy - insurance_taxonomy.csv` – taxonomy (input).
- `annotated_ml_insurance_challenge.csv` – results (output, generated).
- `report.md` – short text report (optional, generated by `report.py`).

---

## 4) How to run (Windows PowerShell)
1. Install dependencies:
```powershell
python -m pip install -r requirements.txt
```
2. Run the classifier (defaults):
```powershell
python main.py
```
3. Generate the quick report (optional):
```powershell
python report.py --input "./annotated_ml_insurance_challenge.csv"
```

---

## 5) Useful parameters (easy tuning)
You can adjust weights and thresholds from the command line:
```powershell
python main.py `
  --companies "./ml_insurance_challenge.csv" `
  --taxonomy "./insurance_taxonomy - insurance_taxonomy.csv" `
  --output "./annotated_ml_insurance_challenge.csv" `
  --weights "0.6,0.4" `                 
  --tfidf-field-weights "0.7,0.3" `     
  --selection "0.2,0.05,5"              
```
Quick explanations:
- `--weights a,b` – how much to trust rules vs TF‑IDF (e.g., 0.7,0.3 = rules matter more).
- `--tfidf-field-weights x,y` – how much to trust description vs tags+meta (e.g., 0.7,0.3 = description matters more).
- `--selection t,m,k` – label selection logic:
  - `t` (min_thr): minimum absolute score (e.g., 0.2).
  - `m` (margin): distance from the best score; labels within this gap are kept (e.g., 0.05).
  - `k` (max_labels): maximum labels per company (e.g., 5).

Tips:
- Want fewer labels (stricter)? increase `t` or decrease `m`.
- Want more labels (looser)? decrease `t` or increase `m`.
- If tags are very good, try `--tfidf-field-weights 0.5,0.5`.

---

## 6) How to review results (fast)
Run the report:
```powershell
python report.py --input "./annotated_ml_insurance_challenge.csv"
```
You’ll get:
- Top assigned labels (see if one label dominates too much).
- `report.md` listing “ambiguous” rows where the top two scores are very close (worth a manual look).

Evaluation checklist:
1. Manually check a sample of 30–50 rows.
2. If you see many generic labels, raise thresholds or upweight description.
3. Inspect ambiguous rows and tweak parameters until you’re happy.

---

## 7) FAQ
– “Why do I sometimes see multiple labels?”
  Because several labels have close scores; we keep the ones near the best so we don’t miss near‑ties.

– “Can I force a single label?”
  Yes: use `--selection 0.2,0.0,1` (margin 0, max 1). Note: you may lose useful info when two labels are truly close.

– “How do I explain a prediction?”
  Check `label_scores_top5`. If you need more, we can add a column with “matched words” quickly.

---

## 8) Limitations and next steps
- TF‑IDF is word‑based: it can miss synonyms (“fire suppression” vs “fire protection”).
  - Improvement: small per‑label synonym lists.
  - Improvement: third signal with sentence embeddings (e.g., `sentence-transformers`).
- Thresholds are heuristic: calibrate on a small manually reviewed set.
- We can gate labels by sector/category to reduce off‑target choices and speed up scoring.

---

## 9) Why this approach fits
- Fast, explainable, no training needed.
- Easy to tune and scales to bigger lists.
- Transparent: keeps scores, offers a simple report, supports a clear presentation.

---

## 10) Output details
- `insurance_label` – selected labels (separated by `;`).
- `label_scores_top5` – top 5 labels with scores (JSON) for QA.

---

## 11) License / usage
For interview evaluation and demonstration purposes.
